{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpkl3OE6LgB+3/qFAbVxee",
      "include_colab_link": true
    },
  "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
  "language_info": {
      "name": "python"
    },
  "accelerator": "GPU",
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArnyWu/DeepGenerativeModels_HW5/blob/main/HW9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers peft bitsandbytes scikit-learn seaborn matplotlib torch accelerate"
      ],
      "metadata": {
        "id": "fbWsXgKK52R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall bitsandbytes\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "d48Xxc0I1DhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFnouFz2r9zy",
        "outputId": "d7f93d58-9124-42c5-d204-3eb4ab6e25d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import re # 用於解析\n",
        "\n",
        "# 抑制不必要的警告\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# --- 參數設定 ---\n",
        "# 使用的模型：Qwen、Gemma\n",
        "GEMMA_MODEL_NAME = \"google/gemma-2-2b\" # For fine-tuning (Sequence Classification)\n",
        "GEMMA_CHAT_MODEL_NAME = \"google/gemma-2-2b-it\" # For zero/few-shot inference\n",
        "NUM_LABELS = 3\n",
        "LABELS = [\"low_risk\", \"mid_risk\", \"high_risk\"]\n",
        "ID2LABEL = {0: \"low_risk\", 1: \"mid_risk\", 2: \"high_risk\"}\n",
        "LABEL2ID = {\"low_risk\": 0, \"mid_risk\": 1, \"high_risk\": 2}\n",
        "TRAIN_EPOCHS = 1 # 保持 1 個 epoch 以便快速執行\n",
        "\n",
        "# 檢查 GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Warning: GPU not available. QLoRA fine-tuning requires a GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. 資料預處理 ---\n",
        "print(\"\\n--- Step 1: Preprocessing ---\")\n",
        "\n",
        "# 載入資料集\n",
        "dataset = load_dataset(\"dair-ai/emotion\")\n",
        "print(\"Dataset loaded:\")\n",
        "print(dataset)\n",
        "\n",
        "# 定義風險映射函式\n",
        "def map_emotion_to_risk(example):\n",
        "    emotion = example['label']\n",
        "    if emotion in [1, 2, 5]:  # joy, love, surprise\n",
        "        example['risk_label'] = 0  # low_risk\n",
        "    elif emotion in [3, 4]:  # anger, fear\n",
        "        example['risk_label'] = 1  # mid_risk\n",
        "    elif emotion == 0:  # sadness\n",
        "        example['risk_label'] = 2  # high_risk\n",
        "    else:\n",
        "        example['risk_label'] = -1\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(map_emotion_to_risk)\n",
        "dataset = dataset.filter(lambda x: x['risk_label'] != -1)\n",
        "dataset = dataset.rename_column(\"risk_label\", \"labels\")\n",
        "\n",
        "print(\"Dataset after risk mapping:\")\n",
        "print(dataset['train'][0])\n",
        "\n",
        "# 載入 Tokenizer (用於 QLoRA) - 使用 Gemma 模型名稱\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize 函式\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=False, max_length=512)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# 準備完整的測試集 (用於所有評估)\n",
        "test_dataset_full_tokenized = tokenized_datasets[\"test\"]\n",
        "original_test_set_full = dataset['test'] # 包含原始 text\n",
        "y_true_test_full = original_test_set_full['labels'] # 統一的真實標籤\n",
        "\n",
        "print(f\"Full test set size: {len(original_test_set_full)}\")\n",
        "\n",
        "# --- 評估用的輔助函式 ---\n",
        "def evaluate_model(y_true, y_pred, y_probs, title_prefix=\"\"):\n",
        "    print(f\"\\n--- {title_prefix} Evaluation ---\")\n",
        "\n",
        "    # F1 Score\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"F1 Score (Weighted): {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    try:\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        # 繪製 CM\n",
        "        fig, ax = plt.subplots()\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)\n",
        "        disp.plot(ax=ax, cmap='Blues')\n",
        "        plt.title(f\"{title_prefix} Confusion Matrix\") # 更新標題\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        cm_filename = f\"plot_cm_{title_prefix.lower().replace(' ', '_').replace('-', '_')}.png\" # 更新檔案名稱\n",
        "        plt.savefig(cm_filename)\n",
        "        print(f\"Saved confusion matrix to {cm_filename}\")\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting confusion matrix: {e}\")\n",
        "\n",
        "    # 需要機率的指標\n",
        "    if y_probs is not None and len(np.unique(y_true)) > 1:\n",
        "        try:\n",
        "            # 標籤二元化\n",
        "            y_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
        "\n",
        "            if y_true_bin.shape[1] == 3 and y_probs.shape[1] != 3:\n",
        "                print(f\"Warning: Probability shape mismatch. Adjusting.\")\n",
        "                temp_probs = np.zeros((y_probs.shape[0], 3))\n",
        "                if y_probs.shape[1] < 3:\n",
        "                     temp_probs[:, :y_probs.shape[1]] = y_probs\n",
        "                y_probs = temp_probs\n",
        "\n",
        "            # AUROC\n",
        "            auroc = roc_auc_score(y_true_bin, y_probs, average='weighted', multi_class='ovr')\n",
        "            print(f\"AUROC (Weighted, OVR): {auroc:.4f}\")\n",
        "\n",
        "            # PR-AUC\n",
        "            pr_auc = average_precision_score(y_true_bin, y_probs, average='weighted')\n",
        "            print(f\"PR-AUC (Weighted): {pr_auc:.4f}\")\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Could not calculate AUROC/PR-AUC: {e}\")\n",
        "    else:\n",
        "        print(\"Skipping AUROC/PR-AUC (no probabilities or only one class present).\")\n",
        "\n",
        "\n",
        "# --- 共用的輔助函式 ---\n",
        "def parse_response(response):\n",
        "    \"\"\"解析模型的生成式回應\"\"\"\n",
        "    response = response.lower()\n",
        "    if \"high_risk\" in response or \"(2)\" in response:\n",
        "        return 2\n",
        "    elif \"mid_risk\" in response or \"(1)\" in response:\n",
        "        return 1\n",
        "    elif \"low_risk\" in response or \"(0)\" in response:\n",
        "        return 0\n",
        "    else:\n",
        "        # 嘗試只找數字\n",
        "        match = re.search(r'\\b([012])\\b', response)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "        else:\n",
        "            return 0 # 解析失敗時，預設為 low_risk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "84e49378f15948f1ace289b6ce88b771",
            "7b4be96467964a5499417b4c89562e8f",
            "e903f6d704244b5b8733adbc1643bbb6",
            "15fae68e9e32425a97af9e6949b5721c",
            "54454c01f4b74b64b5386506c95101bb",
            "ae480431e4534e029008454b529d9349",
            "1d4ae43edc3b4eda8997be96cd70ca83",
            "7213a76e850d44c09c1ab004f1c53907",
            "badaa02face24ce5844321f77cfe8172",
            "8dd79d9f0a894523b126f0db6b4137df",
            "a7ba65745df54961ac076781b1ac3297"
          ]
        },
        "id": "YtbKh91WMDZZ",
        "outputId": "ac88ee7e-c02a-4c0f-e2fd-9cf3078491a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 1: Preprocessing ---\n",
            "Dataset loaded:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n",
            "Dataset after risk mapping:\n",
            "{'text': 'i didnt feel humiliated', 'label': 0, 'labels': 2}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84e49378f15948f1ace289b6ce88b771"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full test set size: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Zero-shot 推論 (使用 Gemma-Chat) ---\n",
        "print(\"\\n--- Step 2: Zero-shot Inference (Gemma-Chat, Full 2000) ---\") # 更新標題\n",
        "print(\"WARNING: This step will run on all 2,000 test samples and will be EXTREMELY SLOW (potentially hours).\")\n",
        "\n",
        "def create_zero_shot_prompt(new_text):\n",
        "    \"\"\"建立 Zero-Shot (無範例) 的 Prompt\"\"\"\n",
        "    prompt = \"This is a text classification task. Classify the text into one of three risk categories: low_risk (0), mid_risk (1), or high_risk (2).\\n\\n\"\n",
        "    prompt += \"===\\n\\n\"\n",
        "    prompt += f\"Text: {new_text}\\nRisk:\"\n",
        "    return prompt\n",
        "\n",
        "try:\n",
        "    # 載入 Gemma-Chat 模型\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(GEMMA_CHAT_MODEL_NAME, trust_remote_code=True) # 使用 Gemma Chat 模型名稱\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        GEMMA_CHAT_MODEL_NAME, # 使用 Gemma Chat 模型名稱\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    if chat_tokenizer.pad_token is None:\n",
        "        chat_tokenizer.pad_token = chat_tokenizer.eos_token\n",
        "\n",
        "    print(f\"Zero-shot model loaded on device: {chat_model.device}\")\n",
        "\n",
        "    y_pred_zero_shot = []\n",
        "\n",
        "    # 迭代完整的 2000 筆資料\n",
        "    for i, test_sample in enumerate(original_test_set_full):\n",
        "        if (i+1) % 50 == 0 or i == 0:\n",
        "            print(f\"Running Zero-shot sample {i+1}/{len(original_test_set_full)}...\")\n",
        "\n",
        "        prompt_text = create_zero_shot_prompt(test_sample['text'])\n",
        "\n",
        "        # 格式化為 Chat (Gemma 的 Chat 格式)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt_text}\n",
        "        ]\n",
        "        text = chat_tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        model_inputs = chat_tokenizer([text], return_tensors=\"pt\").to(chat_model.device)\n",
        "\n",
        "        generated_ids = chat_model.generate(\n",
        "            model_inputs.input_ids,\n",
        "            max_new_tokens=10 # 只需要標籤\n",
        "        )\n",
        "        # Gemma 的 generate 會包含 prompt，需要移除\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        response = chat_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        y_pred_zero_shot.append(parse_response(response))\n",
        "\n",
        "    # 評估 (無機率)\n",
        "    evaluate_model(y_true_test_full, y_pred_zero_shot, y_probs=None, title_prefix=\"Zero-Shot (Gemma-Chat Full)\") # 更新標題\n",
        "\n",
        "    # 清理 VRAM，為下一步做準備\n",
        "    del chat_model\n",
        "    del chat_tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Zero-shot inference: {e}. Skipping.\")\n",
        "    # 確保清理\n",
        "    if 'chat_model' in locals(): del chat_model\n",
        "    if 'chat_tokenizer' in locals(): del chat_tokenizer\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ad2a38cd3efa47b5974ed490ce1c68c5",
            "027bad5e96cc488e8c5066b5e8cf0e95",
            "3c31b18b1467451fbf63358c7b961491",
            "f6bf273c568d4e7a926cc20a20625758",
            "63d5bf67f4934287bfac4dc2bdeece23",
            "8f28fd1f48fe4a30905d1d2c6e717ca5",
            "d0b3f8741bc347c698f590ed30e9dc6a",
            "8f0fa6b3d8824dd0a6b8a0a6a8c7501d",
            "b95471ce64ed43d6a0a3e303acf55057",
            "28bb877d6656491faa8f759e3c4a0577",
            "062d87727ffd4bb5adbf5f71611aa4b0",
            "05c4cd925d344f95a8b9105839a8731b",
            "4afd5e5dffce452cb8a9e4314b33d12e",
            "776cae57b21c4a0089041fef10519a6e",
            "c6b3d3f6b02a4fa9a993b493b250e378",
            "f2cc0954ff10487f9b671d0e5ef0e000",
            "ca858ba3171940ac8ca64452d2dacd59",
            "204c6f0831254dc3bfd554dcb2601c84",
            "7a4da1225a8f430691f842cd2237d089",
            "0ba143b59a484a92b1e178ef3bfd3721",
            "7edb9debe25545ca94d719b44120efc9",
            "2c825ef02cd84686b982cec1ff61c0a5",
            "a48871a980be40d6913bbb851c1ad83b",
            "67e33aebfa984e3a9a352018c2e7174d",
            "3a2dcb2d01904a57907cac204fae86b2",
            "bba02d5ff1b647a18c2da1f02595b7d1",
            "9f153055a6da4881bba8c4fdc82c4fc1",
            "3d0570af692b4a74a3e3ea9064566450",
            "8214a5f278bc4f8f906a84d649a64288",
            "763e087343914e548d9ceac517b18558",
            "3de44272014a4836b3b8585246a55e6f",
            "d3d5f177712b44d9a8432a406448a084",
            "afbc75160aa04250823364025dfa3c2e",
            "925da869492b489b82fca57619cd4995",
            "05104d2a1b45497c868fc0655137a0a4",
            "bc2d851873d24135a649943419a6e514",
            "2fb986d262ea4bf5942a0f85959c2310",
            "7d4a6a5eda2c4a1b946f63eb2fd802a4",
            "1d917338b0f54635a53e4e41b7d320c4",
            "3b05af37d2be440ba68810792f885979",
            "0fdfc59a636440a29cf4f7332641ae82",
            "1fed55b8bba44498af84434148a09b34",
            "8d4f3cea43b04b43aa3a701d94c7f8e9",
            "58ecfd5257524641b1856b57490c2de5",
            "c3520ddcb86e4c1283207a68771fc198",
            "07643a560b5741158eb82b0ed4908fa0",
            "afef50beeb0d48b3bfa41a034df08b49",
            "97c9b75f0d5a4a59a401c961ceac0a84",
            "e34c064f5d7c47fca1e2afd4162abe68",
            "d55dd3bdfc2648ea802ced27cf345e6e",
            "9bec0dd240fa4903bd001930412c87bd",
            "4ef8d494bead4e119a18c681f846ecc3",
            "95eb24a2d4364301b1d92e2d890d22ea",
            "25010a45dbc84c71bf283f20da351eae",
            "a493c158623a48a8bb92b654ff75ba3f",
            "7b69d6dfe4964543868f7807ee28348d",
            "d79d9c88acb843f9b3513951bc1bfa95",
            "c231754b7f2d47f086b1cd0a916b6042",
            "5231bcce07884ea58a9bd9d280cfa15f",
            "e8defd56d4be4e289e9e38293903566e",
            "a98d363135824a358a38f6eb0d53bb50",
            "4402d612722f4cdb9e91cb5c191662be",
            "b42b8c35c7934ef9a319f19240c8dc87",
            "cf4c34343cd64db1bdec9001e58578f1",
            "3f29090a2dc1497db17449b73aad5cac",
            "eccb04d516d543418ba9482355e91b8d",
            "5d2b983c18c949349d8aca47c1bd42a0",
            "7ae36b52ac7b43e787f7dee3f76909e3",
            "297a1f992a9e47ba9062befda1102629",
            "4365365e58d54ef39dc0a82aa5740cf6",
            "e2dea9a5004247d1bace444b8499b92a",
            "16e06f2c598a4c78a40070f1d4b085b2",
            "0d7fb35ab28a41a8997387ac0f4afdbf",
            "13a228702f634afeaef0c3d393072f03",
            "24c62850acdd4515bfa6a6a6788c95ed",
            "dcc81447a9bc43a2a8218a6e375ab296",
            "e402700e41cd48f189266cc1d626a901",
            "9e85d7d2f2444b0589e0897d755844a8",
            "937f40c0077c483c86c4d4598405ce23",
            "5820b42b574b4cf4845961c230d5fb1e",
            "5d2095af983e45609b098d678fc235df",
            "867731523d864699a0a02e60f84851cd",
            "aee324b496634a09820aba202b7a7e0e",
            "6b77c6375a1f4ed681d8d0d9c8e57190",
            "c1e583cd639f470ab4f193d55881ddd7",
            "4f0ef4fc4ea0472cbd0bf94fcdd52ac0",
            "812d2f6bde7b484aa3da95042a7a185e",
            "7f9ebd9553e042efa9306f9050ce3dda",
            "d0de51a091434ef5bb02a929f26a2145",
            "56c90037b4ef4caba90e7ffd93763e7b",
            "788f74aac53f47649dbca4d946329264",
            "72730824b5834e0396419b2884f35a7c",
            "4f69b720eb9243a98ceb6fce7e6f6cb4",
            "05b6ce8fbd5949daa4091bf9221674fa",
            "e6ff7e2e72c5434ba3aae6bb63462bb0",
            "b473bc077d8b4cc0b8df7ee40f0509e7",
            "b6c79c71409e44f6a8e8d6483551f879",
            "40b60666a1a54bdc999bf8e26ca2655a",
            "2aaca091b908486495fe4a19482fed5e",
            "6e3d7d6bc2254f9d9ccaf9176808eed7",
            "6aa8aec6f6424732b284a283e8433b0d",
            "2722b66189204dbf9fdd4c43a6d55598",
            "6481e640221f40d0b423bd48bdd6f3e0",
            "51cb03b0ff404b88a7de26b09957ef3c",
            "7ba78f09b2de4ef8b1e9588606aee88a",
            "505ba5e894ef460586fd7162ee8ded75",
            "1397259faff84625b71c0da808a6e0e5",
            "575d17afdf044df8a270da92f2036bcc",
            "bce28b40620b4eeaa4db37dc2b89b4a5",
            "33c621f72e1b4d2d8046d3efab492b56",
            "62b0e584729a4c1291cf66e3d916771d",
            "6533c9aa471a43f8a8ced20e392f9178",
            "f4b627c298b042af960ccd6a8b63c80c",
            "3decdba05edf4f4a8ec4d0c1deaf9dc1",
            "40e9d74023b344dea43fab8c6b069c14",
            "ea1dcbf1380d4bab980b16014efc628a",
            "37365a68b1474cc4967afbba376d4537",
            "91632f24ec8f416d9af3975e9b4fc6a9",
            "33ca0aa1cfa34f1dbfb3bd0df5d62485",
            "93debb64a02043e08ab95129db525fe6",
            "19f655524f304ec48f8f5917c58ea7e4"
          ]
        },
        "id": "lhpQ-1AqMHz6",
        "outputId": "b156b9af-8360-4009-ef46-98cec55b6b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 2: Zero-shot Inference (Gemma-Chat, Full 2000) ---\n",
            "WARNING: This step will run on all 2,000 test samples and will be EXTREMELY SLOW (potentially hours).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad2a38cd3efa47b5974ed490ce1c68c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05c4cd925d344f95a8b9105839a8731b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a48871a980be40d6913bbb851c1ad83b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "925da869492b489b82fca57619cd4995"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3520ddcb86e4c1283207a68771fc198"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b69d6dfe4964543868f7807ee28348d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d2b983c18c949349d8aca47c1bd42a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e85d7d2f2444b0589e0897d755844a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0de51a091434ef5bb02a929f26a2145"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e3d7d6bc2254f9d9ccaf9176808eed7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62b0e584729a4c1291cf66e3d916771d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot model loaded on device: cuda:0\n",
            "Running Zero-shot sample 1/2000...\n",
            "Running Zero-shot sample 50/2000...\n",
            "Running Zero-shot sample 100/2000...\n",
            "Running Zero-shot sample 150/2000...\n",
            "Running Zero-shot sample 200/2000...\n",
            "Running Zero-shot sample 250/2000...\n",
            "Running Zero-shot sample 300/2000...\n",
            "Running Zero-shot sample 350/2000...\n",
            "Running Zero-shot sample 400/2000...\n",
            "Running Zero-shot sample 450/2000...\n",
            "Running Zero-shot sample 500/2000...\n",
            "Running Zero-shot sample 550/2000...\n",
            "Running Zero-shot sample 600/2000...\n",
            "Running Zero-shot sample 650/2000...\n",
            "Running Zero-shot sample 700/2000...\n",
            "Running Zero-shot sample 750/2000...\n",
            "Running Zero-shot sample 800/2000...\n",
            "Running Zero-shot sample 850/2000...\n",
            "Running Zero-shot sample 900/2000...\n",
            "Running Zero-shot sample 950/2000...\n",
            "Running Zero-shot sample 1000/2000...\n",
            "Running Zero-shot sample 1050/2000...\n",
            "Running Zero-shot sample 1100/2000...\n",
            "Running Zero-shot sample 1150/2000...\n",
            "Running Zero-shot sample 1200/2000...\n",
            "Running Zero-shot sample 1250/2000...\n",
            "Running Zero-shot sample 1300/2000...\n",
            "Running Zero-shot sample 1350/2000...\n",
            "Running Zero-shot sample 1400/2000...\n",
            "Running Zero-shot sample 1450/2000...\n",
            "Running Zero-shot sample 1500/2000...\n",
            "Running Zero-shot sample 1550/2000...\n",
            "Running Zero-shot sample 1600/2000...\n",
            "Running Zero-shot sample 1650/2000...\n",
            "Running Zero-shot sample 1700/2000...\n",
            "Running Zero-shot sample 1750/2000...\n",
            "Running Zero-shot sample 1800/2000...\n",
            "Running Zero-shot sample 1850/2000...\n",
            "Running Zero-shot sample 1900/2000...\n",
            "Running Zero-shot sample 1950/2000...\n",
            "Running Zero-shot sample 2000/2000...\n",
            "\n",
            "--- Zero-Shot (Gemma-Chat Full) Evaluation ---\n",
            "F1 Score (Weighted): 0.4130\n",
            "Confusion Matrix:\n",
            "[[828  62  30]\n",
            " [358  69  72]\n",
            " [457  39  85]]\n",
            "Saved confusion matrix to plot_cm_zero_shot_(gemma_chat_full).png\n",
            "Skipping AUROC/PR-AUC (no probabilities or only one class present).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Few-shot 推論 (使用 Gemma-Chat, 平衡範例) ---\n",
        "print(\"\\n--- Step 3: Few-shot Inference (Gemma-Chat, Full 2000) ---\") # 更新標題\n",
        "print(\"INFO: Using balanced 6-shot examples (one from each emotion).\")\n",
        "print(\"WARNING: This step will also be EXTREMELY SLOW (potentially hours).\")\n",
        "\n",
        "# 需要 concatenate_datasets 來組合\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "# 我們使用原始的文字資料集\n",
        "original_train_set = dataset['train']\n",
        "\n",
        "# 「平衡抽樣」\n",
        "# 原始標籤: 0: sadness, 1: joy, 2: love, 3: anger, 4: fear, 5: surprise\n",
        "print(\"Selecting 6 balanced few-shot examples...\")\n",
        "example_list = []\n",
        "for i in range(6):\n",
        "    # 從原始標籤 (label) 過濾，確保每種情緒都有\n",
        "    example_list.append(\n",
        "        original_train_set.filter(lambda x: x['label'] == i).select(range(1))\n",
        "    )\n",
        "\n",
        "# 將 6 個範例組合起來\n",
        "few_shot_examples = concatenate_datasets(example_list)\n",
        "print(\"Balanced examples selected:\")\n",
        "print(few_shot_examples['text'])\n",
        "print(few_shot_examples['labels']) # 應該會顯示 [2, 0, 0, 1, 1, 0] (對應的 risk)\n",
        "\n",
        "def create_few_shot_prompt(examples, new_text):\n",
        "    \"\"\"建立 Few-Shot (有範例) 的 Prompt\"\"\"\n",
        "    prompt = \"This is a text classification task. Classify the text into one of three risk categories: low_risk (0), mid_risk (1), or high_risk (2).\\n\\n\"\n",
        "    # 現在 'examples' 會有 6 筆\n",
        "    for ex in examples:\n",
        "        prompt += f\"Text: {ex['text']}\\nRisk: {ID2LABEL[ex['labels']]} ({ex['labels']})\\n\\n\"\n",
        "    prompt += \"===\\n\\n\"\n",
        "    prompt += f\"Text: {new_text}\\nRisk:\"\n",
        "    return prompt\n",
        "\n",
        "try:\n",
        "    # 載入 Gemma-Chat 模型\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(GEMMA_CHAT_MODEL_NAME, trust_remote_code=True) # 使用 Gemma Chat 模型名稱\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        GEMMA_CHAT_MODEL_NAME, # 使用 Gemma Chat 模型名稱\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    if chat_tokenizer.pad_token is None:\n",
        "        chat_tokenizer.pad_token = chat_tokenizer.eos_token\n",
        "\n",
        "    print(f\"Few-shot model loaded on device: {chat_model.device}\")\n",
        "\n",
        "    y_pred_few_shot = []\n",
        "\n",
        "    # 迭代完整的 2000 筆資料\n",
        "    for i, test_sample in enumerate(original_test_set_full):\n",
        "        if (i+1) % 50 == 0 or i == 0:\n",
        "            print(f\"Running Few-shot sample {i+1}/{len(original_test_set_full)}...\")\n",
        "\n",
        "        prompt_text = create_few_shot_prompt(few_shot_examples, test_sample['text'])\n",
        "\n",
        "        # 格式化為 Chat (Gemma 的 Chat 格式)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt_text}\n",
        "        ]\n",
        "        text = chat_tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        model_inputs = chat_tokenizer([text], return_tensors=\"pt\").to(chat_model.device)\n",
        "\n",
        "        generated_ids = chat_model.generate(\n",
        "            model_inputs.input_ids,\n",
        "            max_new_tokens=10 # 只需要標籤\n",
        "        )\n",
        "        # Gemma 的 generate 會包含 prompt，需要移除\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        response = chat_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        y_pred_few_shot.append(parse_response(response))\n",
        "\n",
        "    # 評估 (無機率)\n",
        "    evaluate_model(y_true_test_full, y_pred_few_shot, y_probs=None, title_prefix=\"Few-Shot (Gemma-Chat Full 6-Shot Balanced)\") # 更新標題\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Few-shot inference: {e}. Skipping.\")\n",
        "finally:\n",
        "    # 清理 VRAM\n",
        "    if 'chat_model' in locals(): del chat_model\n",
        "    if 'chat_tokenizer' in locals(): del chat_tokenizer\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6b9fa8c2060c47958706198a001eebbe",
            "9bbe7443e4194d4b854ed3cac4c6b43a",
            "2ad2832b9cc84f83a9b8064abe80f2ab",
            "892686f66e27440caf297ef57cebb0ba",
            "65ab9fb7484d4639b059c19f5bafd795",
            "52a46751d98d43dea3e0658c9ccd277a",
            "5530af78109544ac91a7f1f4f2815dfc",
            "ca381404703d4f019c76787161baf81d",
            "08de6a04dd234282be4c82e123472fdf",
            "5fbdfd96cb5f4fb29860e3a2daf08fd3",
            "652af863765b4ace8b0e10c6705d0683",
            "a943632aae864ba6aeb9fe773ad262d2",
            "524b1ce368f843f29f907f560c4130c2",
            "297399c1f29e40539e3e58f28fd5c5c5",
            "2fd031d800c04555993a94611814e961",
            "97a7896f0051461c92223241db53b215",
            "4e62d21f643a464394da6f193475d9c0",
            "9da262101dc14f08b9a1060741b165d7",
            "f6e216813aab43fb81189b4fb0c43f24",
            "e5b857bec54944f4beb5b11216d26570",
            "99a22927e18d415daa3a8f2b55ac157b",
            "8736cd9a99c641f99fed2a7af1bd0f64",
            "393a1711abc340738331500211ccf6f3",
            "be75ae199b11446fb73d83fe415bf283",
            "b6da30a1b31c4b9e8694ab9b02d0b364",
            "bf23782d24bd4559b3b04e79235fade0",
            "4ddb3283f32b47509ccd8e89e514188c",
            "89d3792dfa7844f4b22cde5332af495d",
            "13daacd8487f4a39abe1d47dc53b715f",
            "4b7771bb343448dc99549c7b7bbf0240",
            "f09f0ae04052461f9aaf886b174667a2",
            "88fafd7e6ad34c0489c12e0e560b26b9",
            "054f7c308e5741c9b1d147901262eb18",
            "905cf3158dee412fbd62f0df1768d94a",
            "bdaf4e8233ce4173af1c6eb60346cda2",
            "b1a0ad75a5f948b78b4ba8711c7e3735",
            "2ba63a50c6d64a64b7fddf25d5d26231",
            "3d5e429a99d14f1e953b09cd3681a696",
            "661bb5929da94549a408ce7934d3a8e2",
            "19c36e5b71734620a85bdb5c022a6c58",
            "8b12f99137d64708b25aa8c1112d7880",
            "6bd885ae0e5d4c6482205d551f9e03fc",
            "6a93c5d1c4714591bbe6c7673b73ab9a",
            "2b5d387d55804207bd46c2b6976f84ab",
            "bfe448c7918d40e29d0f1876bd45f624",
            "fdf8db0506d6442cab59aaab13c96240",
            "69596354d6454316bfd2441b438cd36b",
            "8a3ac57cb3c04b68b465c5c985a2e9aa",
            "65e555b5968449188f26aca57be725a9",
            "440faac311f843248b541d0da7873d09",
            "630726eef9534d9291591df821bff3f6",
            "96f2cda722d44cdbae3f94a3fbca40b3",
            "5b28ad449a804cf483ff69aa807cc869",
            "1c68353c715d4d5a80d812185bf891be",
            "93223c7c72964658ae5c839a8c4a2eeb",
            "4ce79ce7213241c1830f95fe4f36e4da",
            "2f46c0eab4584c8b81dc326bee111df4",
            "74358977c09d408ba09825fb5dd87383",
            "fb9cefe7dcb14976b1ce8959e26d8d3a",
            "1df0fd8cf0624a4f9aa94d363bbb4df2",
            "1cc1d8304dc148939c94b3324b156d1c",
            "b9859e03ba214cd8a7969c2a8972eaac",
            "af4c459931fd482a855fcd29182744fa",
            "4cd7203b86b84445b866e41126ca2ad7",
            "b6fe1ad737974abdb3429324e41ca761",
            "0f7fbabab3414153b2afbb2ed3d8df11",
            "c16624cbc4cd4bbc8bca2a173313ae94",
            "82766b3afac1438590fe62e3284deed1",
            "18d6d2d36d044a15995cf425545deb92",
            "281ede2c5121448ab5a788f88973049a",
            "2d736696cadf4e5ea5e431ed66f97247",
            "2a9ab9bafd8649faac8fb2b5ac9532a8",
            "de3e9c3c2b4d4a67bb8972526d1688ee",
            "50699b2ce4d54a0ab06a7baf3c7a4994",
            "0beba70f3b7747d8b1e5ccde2cdcd5b3",
            "113b13cb76ee4ede9b1c3cf2028749fc",
            "9e7d492f7eff432199a95f2a7a6bc48b"
          ]
        },
        "id": "N5o_A-6aMMld",
        "outputId": "6188ed5c-1970-47de-98ed-2439017b7e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 3: Few-shot Inference (Gemma-Chat, Full 2000) ---\n",
            "INFO: Using balanced 6-shot examples (one from each emotion).\n",
            "WARNING: This step will also be EXTREMELY SLOW (potentially hours).\n",
            "Selecting 6 balanced few-shot examples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/16000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b9fa8c2060c47958706198a001eebbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/16000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a943632aae864ba6aeb9fe773ad262d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/16000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "393a1711abc340738331500211ccf6f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/16000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "905cf3158dee412fbd62f0df1768d94a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/16000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfe448c7918d40e29d0f1876bd45f624"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/16000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ce79ce7213241c1830f95fe4f36e4da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced examples selected:\n",
            "Column(['i didnt feel humiliated', 'i have been with petronas for years i feel that petronas has performed well and made a huge profit', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'im grabbing a minute to post i feel greedy wrong', 'i feel as confused about life as a teenager or as jaded as a year old man'])\n",
            "Column([2, 0, 0, 1, 1])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c16624cbc4cd4bbc8bca2a173313ae94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-shot model loaded on device: cuda:0\n",
            "Running Few-shot sample 1/2000...\n",
            "Running Few-shot sample 50/2000...\n",
            "Running Few-shot sample 100/2000...\n",
            "Running Few-shot sample 150/2000...\n",
            "Running Few-shot sample 200/2000...\n",
            "Running Few-shot sample 250/2000...\n",
            "Running Few-shot sample 300/2000...\n",
            "Running Few-shot sample 350/2000...\n",
            "Running Few-shot sample 400/2000...\n",
            "Running Few-shot sample 450/2000...\n",
            "Running Few-shot sample 500/2000...\n",
            "Running Few-shot sample 550/2000...\n",
            "Running Few-shot sample 600/2000...\n",
            "Running Few-shot sample 650/2000...\n",
            "Running Few-shot sample 700/2000...\n",
            "Running Few-shot sample 750/2000...\n",
            "Running Few-shot sample 800/2000...\n",
            "Running Few-shot sample 850/2000...\n",
            "Running Few-shot sample 900/2000...\n",
            "Running Few-shot sample 950/2000...\n",
            "Running Few-shot sample 1000/2000...\n",
            "Running Few-shot sample 1050/2000...\n",
            "Running Few-shot sample 1100/2000...\n",
            "Running Few-shot sample 1150/2000...\n",
            "Running Few-shot sample 1200/2000...\n",
            "Running Few-shot sample 1250/2000...\n",
            "Running Few-shot sample 1300/2000...\n",
            "Running Few-shot sample 1350/2000...\n",
            "Running Few-shot sample 1400/2000...\n",
            "Running Few-shot sample 1450/2000...\n",
            "Running Few-shot sample 1500/2000...\n",
            "Running Few-shot sample 1550/2000...\n",
            "Running Few-shot sample 1600/2000...\n",
            "Running Few-shot sample 1650/2000...\n",
            "Running Few-shot sample 1700/2000...\n",
            "Running Few-shot sample 1750/2000...\n",
            "Running Few-shot sample 1800/2000...\n",
            "Running Few-shot sample 1850/2000...\n",
            "Running Few-shot sample 1900/2000...\n",
            "Running Few-shot sample 1950/2000...\n",
            "Running Few-shot sample 2000/2000...\n",
            "\n",
            "--- Few-Shot (Gemma-Chat Full 6-Shot Balanced) Evaluation ---\n",
            "F1 Score (Weighted): 0.4903\n",
            "Confusion Matrix:\n",
            "[[466 360  94]\n",
            " [ 58 318 123]\n",
            " [ 95 310 176]]\n",
            "Saved confusion matrix to plot_cm_few_shot_(gemma_chat_full_6_shot_balanced).png\n",
            "Skipping AUROC/PR-AUC (no probabilities or only one class present).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. LoRA / QLoRA Fine-tuning (使用 Gemma-Base) ---\n",
        "print(\"\\n--- Step 4: LoRA / QLoRA Fine-tuning (Gemma-Base) ---\") # 更新標題\n",
        "try:\n",
        "    # QLoRA 設定 (4-bit 量化)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # 載入基底模型 (用於序列分類) - 使用 Gemma Base 模型名稱\n",
        "    qlora_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        GEMMA_MODEL_NAME, # 使用 Gemma Base 模型\n",
        "        num_labels=NUM_LABELS,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        id2label=ID2LABEL,\n",
        "        label2id=LABEL2ID,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    qlora_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # LoRA 設定\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        # 鎖定 Gemma 的 attention 模組\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"gemma_act\"], # 更新 target_modules\n",
        "    )\n",
        "\n",
        "    # 套用 PEFT (LoRA)\n",
        "    peft_model = get_peft_model(qlora_model, lora_config)\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "    # Data Collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # 訓練參數\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gemma-lora-emotion-risk\", # 更新輸出目錄\n",
        "        learning_rate=2e-4, # LoRA 可用較高學習率\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=TRAIN_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        logging_steps=100,\n",
        "        report_to=\"none\", # 關閉 wandb\n",
        "        bf16=True if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else False,\n",
        "    )\n",
        "\n",
        "    # 建立 Trainer\n",
        "    trainer = Trainer(\n",
        "        model=peft_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # --- 5. (隱含) & 6. 訓練與評估 ---\n",
        "    print(\"Starting QLoRA fine-tuning...\")\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning complete.\")\n",
        "\n",
        "    print(\"\\n--- Step 6: Evaluating QLoRA Model ---\") # 更新標題\n",
        "\n",
        "    # 在測試集上預測\n",
        "    predictions = trainer.predict(test_dataset_full_tokenized) # 使用 tokenized 測試集\n",
        "\n",
        "    # 處理預測結果\n",
        "    y_pred_lora = np.argmax(predictions.predictions, axis=1)\n",
        "    y_probs_lora_logits = torch.from_numpy(predictions.predictions)\n",
        "    y_probs_lora = torch.nn.functional.softmax(y_probs_lora_logits, dim=1).numpy()\n",
        "    y_true_lora = predictions.label_ids # 這會等於 y_true_test_full\n",
        "\n",
        "    # 評估\n",
        "    evaluate_model(y_true_lora, y_pred_lora, y_probs_lora, title_prefix=\"QLoRA Fine-Tune (Gemma-Base)\") # 更新標題\n",
        "\n",
        "\n",
        "    # --- 7. 視覺化呈現 (使用 QLoRA 模型的結果) ---\n",
        "    print(\"\\n--- Step 7: Visualizing QLoRA Results ---\") # 更新標題\n",
        "\n",
        "    # 取得 high_risk (label 2) 的機率\n",
        "    p_high_risk = y_probs_lora[:, 2]\n",
        "\n",
        "    # 圖 1: 高風險走勢圖\n",
        "    try:\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.plot(p_high_risk, alpha=0.7, label=\"P(high_risk)\")\n",
        "        plt.title(\"High Risk Probability (P(high_risk)) Trend (Gemma-Base QLoRA Model)\") # 更新標題\n",
        "        plt.xlabel(\"Test Sample Index\")\n",
        "        plt.ylabel(\"P(high_risk)\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.savefig(\"plot_high_risk_trend_gemma.png\") # 更新檔案名稱\n",
        "        print(\"Saved high-risk trend plot to plot_high_risk_trend_gemma.png\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting trend: {e}\")\n",
        "\n",
        "    # 圖 2.1: 高風險滾動平均 (Rolling Window)\n",
        "    try:\n",
        "        # 使用 pandas 進行滾動平均\n",
        "        rolling_avg = pd.Series(p_high_risk).rolling(window=50, min_periods=1).mean()\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.plot(rolling_avg, color='red')\n",
        "        plt.title(\"Rolling Average (Window=50) of P(high_risk) (Gemma-Base QLoRA Model)\") # 更新標題\n",
        "        plt.xlabel(\"Test Sample Index\")\n",
        "        plt.ylabel(\"Rolling Avg P(high_risk)\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.savefig(\"plot_high_risk_rolling_avg_gemma.png\") # 更新檔案名稱\n",
        "        print(\"Saved high-risk rolling average plot to plot_high_risk_rolling_avg_gemma.png\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting rolling average: {e}\")\n",
        "\n",
        "    # 圖 2.2: 高風險濃度熱圖 (1D Heatmap)\n",
        "    try:\n",
        "        plt.figure(figsize=(18, 2)) # 寬而短\n",
        "        sns.heatmap(\n",
        "            [p_high_risk],\n",
        "            cmap=\"rocket\",\n",
        "            cbar=True,\n",
        "            cbar_kws={'label': 'P(high_risk)', 'orientation': 'horizontal', 'pad': 0.3},\n",
        "            xticklabels=False,\n",
        "            yticklabels=False\n",
        "        )\n",
        "        plt.title(\"High Risk Probability Concentration (Gemma-Base QLoRA Model)\") # 更新標題\n",
        "        plt.xlabel(\"Test Sample Index\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"plot_high_risk_concentration_heatmap_gemma.png\") # 更新檔案名稱\n",
        "        print(\"Saved high-risk concentration heatmap to plot_high_risk_concentration_heatmap_gemma.png\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting heatmap: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during QLoRA fine-tuning or evaluation: {e}\")\n",
        "    print(\"This may be due to CUDA OOM or other resource constraints.\")\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854,
          "referenced_widgets": [
            "4a5fe3e9a4974058b431f0afce6bfa31",
            "533ee53e891547fab04a290d10a5693d",
            "e99e0585b6c44173bd4a54f3bd54e772",
            "507c147b290e471589af0a433802f928",
            "36268058cecb4eec9160df04a5fe9677",
            "fb6f6cadf74d474b9acedcfec780bc73",
            "46635bb1b62145cd981027fed78a94d0",
            "e4484d9824c8473eaf8fc14b3e66a923",
            "0f989e3586d74c4e80eb44fb2d99f0d9",
            "baa1826f0c30463f98bacd0217e1e9fa",
            "c82826a9b6af4af8812f94cd37138a11",
            "0516e6bb28a34f07a632eddd2b40e182",
            "02d0b04e029d4073ad8165cac282a664",
            "eb48b974c0d148c88cb766defd31d7d4",
            "f7a4c4e5cef045ddb111c672258033d0",
            "dd78265a74c948debf94af24057b5542",
            "75783cb9e5e5412b84565c57d60338fe",
            "9dac3ec650ae4a349b7484bf862fa96a",
            "173168effbed4aa8b76e6ad42fa2b36d",
            "fcfd99102ed84018bf2d6559b70bd045",
            "8f748bbe7a674d548c2a1abf3ac62a13",
            "00632da5d99844d29a248c3b016a53e1",
            "de0da46873424f6fbd6f5769af2e7023",
            "3f4f895714b542e8a7399951ff914807",
            "46956382ff0e487f9e1f2ff7a107c4b7",
            "a6385659afb24477b5b9d56260fb76f9",
            "bae6707308f64cdbbf6143548f160682",
            "b520d45161de4410b0e91bd342c2fef8",
            "caf38d9702e64a43b00b2e34795307b7",
            "251e1f248638418da52645f4da4970bd",
            "480757f2161e455ba9b277caa2d42f66",
            "5590a177ae8c4cdd9e3967ca81bc526e",
            "9ca1a594c4464ae796da2f756372683d",
            "0a2e00dc54f640a8b0a69042d9995c20",
            "f734ebef65e34006a51f013e25d7334f",
            "cc967a25950f4ff8b389f0f2892005e4",
            "eb0a0d30032d4f84932bf8fb8ad58fcc",
            "d960e36a220943879bf09a9ce73b34a8",
            "0ed97619a632439a8e2b99759fe8846b",
            "bfb5b32bc87749e9b129202b0966d032",
            "eaee3dca3a7142e8a387e045bdad9a24",
            "800e1c78ffa44016972c686794f8d07d",
            "5d059befa68e4fbd802f73d136b2f3e3",
            "d113eb195f6041b6ab2317b99cbec666",
            "ff9d9e75e58e4c56bf96b575e76bf6ba",
            "f47c7aad621c40178488282c686c7895",
            "42b8c7a6367749db9d958d980fbf8a2f",
            "ad1da4594e8c493aa1aa9dbb8d662c26",
            "a682425a058845839806b012592cfe26",
            "b970c1ad9d2e40009e7131a8e6d2e8b1",
            "950f56e55a574e27870a385f6a0af4f6",
            "511ca01990f240aa9b138f82c7dac1bd",
            "082d561f0357422881441dfb89904da5",
            "3e8b320e97ec4193834971ab453948f5",
            "ff1599e70e1f45b3b0509974611c7a55",
            "91ff6c61f8eb4ba5b31c0d4d346d5e41",
            "8cc211489db34c59ab2b32ebbfcfc0ef",
            "00cf40fe7f924e8da2150f2251aa7540",
            "9babdbe8bb88408293d44d622f6ce272",
            "82150b30e7e642758881eb1b18fd87fb",
            "9df35fb0ae1549cd87e9acc63a59c356",
            "d96f10a4d25a41188dd499799d623211",
            "a1efa99d95994707a9fa5cc45acdb536",
            "eb4d7534274a43fab30692bb8c352dd7",
            "c1f265701e7543ed9b37870e6511ad26",
            "c57cffab8ada45469d487b7b8c237882"
          ]
        },
        "id": "53qOqlr5O1dV",
        "outputId": "50b53326-e780-4f46-b144-e6e1ef9f9d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 4: LoRA / QLoRA Fine-tuning (Gemma-Base) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a5fe3e9a4974058b431f0afce6bfa31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0516e6bb28a34f07a632eddd2b40e182"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de0da46873424f6fbd6f5769af2e7023"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a2e00dc54f640a8b0a69042d9995c20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff9d9e75e58e4c56bf96b575e76bf6ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91ff6c61f8eb4ba5b31c0d4d346d5e41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-2b and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 20,773,632 || all params: 2,635,122,432 || trainable%: 0.7883\n",
            "Starting QLoRA fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 53:49, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.066400</td>\n",
              "      <td>0.063770</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning complete.\n",
            "\n",
            "--- Step 6: Evaluating QLoRA Model ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- QLoRA Fine-Tune (Gemma-Base) Evaluation ---\n",
            "F1 Score (Weighted): 0.9695\n",
            "Confusion Matrix:\n",
            "[[901  15   4]\n",
            " [ 11 471  17]\n",
            " [  2  12 567]]\n",
            "Saved confusion matrix to plot_cm_qlora_fine_tune_(gemma_base).png\n",
            "AUROC (Weighted, OVR): 0.9988\n",
            "PR-AUC (Weighted): 0.9972\n",
            "\n",
            "--- Step 7: Visualizing QLoRA Results ---\n",
            "Saved high-risk trend plot to plot_high_risk_trend_gemma.png\n",
            "Saved high-risk rolling average plot to plot_high_risk_rolling_avg_gemma.png\n",
            "Saved high-risk concentration heatmap to plot_high_risk_concentration_heatmap_gemma.png\n",
            "\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    }
  ]
}
